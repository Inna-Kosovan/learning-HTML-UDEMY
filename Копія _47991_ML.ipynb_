{"cells":[{"cell_type":"markdown","metadata":{"id":"7s6PJOwmqnxK"},"source":["Сохранить на Google Диске . Вы можете сохранить файл Python на Google Диске, а затем загружать его оттуда при необходимости.\n","\n","Чтобы сохранить на Google Диск:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1697647538390,"user":{"displayName":"Кондратов Олег","userId":"05121404142611870598"},"user_tz":-120},"id":"Z6SWXR8JvbDq","outputId":"f19d480b-f0a0-4522-885d-f2ca8196e38c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing /content/drive/MyDrive/BINANCE/1H/1h_data_processor.py\n"]}],"source":["%%writefile /content/drive/MyDrive/BINANCE/1H/1h_data_processor.py"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17930,"status":"ok","timestamp":1698921822536,"user":{"displayName":"Oleh Kondracki","userId":"06214181816493942618"},"user_tz":-60},"id":"_Tsn0N6vHm20","outputId":"93285333-02b4-44bf-9f17-7222d911d6ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ghx-dibHz83o"},"outputs":[],"source":["!pip install pandas scikit-learn tensorflow\n","!pip install matplotlib\n","!pip install tensorflow\n","!pip install scikit-learn\n","!pip install keras\n","!pip install ta\n","!pip install xgboost\n","!pip install joblib\n","\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2013,"status":"ok","timestamp":1698921901971,"user":{"displayName":"Oleh Kondracki","userId":"06214181816493942618"},"user_tz":-60},"id":"kT4CAPPV5Pj3","outputId":"3b5f6e7f-2421-42b2-b5f5-f3551edc0bcc"},"outputs":[{"output_type":"stream","name":"stdout","text":["            EVENT_TIME    TRADE_ID     PRICE  QUANTITY  BUYER_ORDER_ID  \\\n","0  2023-10-10 11:37:37  3234595905  27565.79   0.00036     22680259990   \n","1  2023-10-10 11:37:37  3234595869  27564.88   0.00049     22680259809   \n","2  2023-10-10 11:37:37  3234595868  27564.88   0.00036     22680259809   \n","3  2023-10-10 11:37:37  3234595867  27564.84   0.00025     22680259809   \n","4  2023-10-10 11:37:37  3234595866  27564.82   0.00022     22680259809   \n","5  2023-10-10 11:37:37  3234595865  27564.81   0.00036     22680259809   \n","6  2023-10-10 11:37:37  3234595864  27564.80   0.00033     22680259809   \n","7  2023-10-10 11:37:37  3234595863  27564.74   0.00036     22680259809   \n","8  2023-10-10 11:37:37  3234595862  27564.68   0.00033     22680259809   \n","9  2023-10-10 11:37:37  3234595861  27564.68   0.00025     22680259809   \n","\n","   SELLER_ORDER_ID  IS_BUYER_MARKET_MAKER     EMA_short     EMA_long  RSI  \\\n","0      22680258817                      0      0.000000      0.00000  0.0   \n","1      22680259260                      0      0.000000      0.00000  0.0   \n","2      22680258785                      0      0.000000      0.00000  0.0   \n","3      22680258803                      0      0.000000      0.00000  0.0   \n","4      22680258597                      0  27565.030864      0.00000  0.0   \n","5      22680258778                      0  27564.957243      0.00000  0.0   \n","6      22680258865                      0  27564.904829      0.00000  0.0   \n","7      22680258761                      0  27564.849886      0.00000  0.0   \n","8      22680258840                      0  27564.793257      0.00000  0.0   \n","9      22680258765                      0  27564.755505  27564.92651  0.0   \n","\n","       OBV  VWAP       ATR  Parabolic_SAR  \n","0  0.00036   0.0  0.000000   27565.790000  \n","1 -0.00013   0.0  0.000000   27564.880000  \n","2  0.00023   0.0  0.000000   27565.790000  \n","3 -0.00002   0.0  0.000000   27565.771800  \n","4 -0.00024   0.0  0.194000   27565.734528  \n","5 -0.00060   0.0  0.157200   27565.679656  \n","6 -0.00093   0.0  0.127760   27565.610084  \n","7 -0.00129   0.0  0.114208   27565.529075  \n","8 -0.00162   0.0  0.103366   27565.434386  \n","9 -0.00137   0.0  0.082693   27565.328772  \n","Total Number of Rows: 47991\n","Total Number of Columns: 14\n","EVENT_TIME                object\n","TRADE_ID                   int64\n","PRICE                    float64\n","QUANTITY                 float64\n","BUYER_ORDER_ID             int64\n","SELLER_ORDER_ID            int64\n","IS_BUYER_MARKET_MAKER      int64\n","EMA_short                float64\n","EMA_long                 float64\n","RSI                      float64\n","OBV                      float64\n","VWAP                     float64\n","ATR                      float64\n","Parabolic_SAR            float64\n","dtype: object\n","The DataFrame has 47991 rows and 14 columns.\n","Columns in df: Index(['EVENT_TIME', 'TRADE_ID', 'PRICE', 'QUANTITY', 'BUYER_ORDER_ID',\n","       'SELLER_ORDER_ID', 'IS_BUYER_MARKET_MAKER', 'EMA_short', 'EMA_long',\n","       'RSI', 'OBV', 'VWAP', 'ATR', 'Parabolic_SAR'],\n","      dtype='object')\n"]}],"source":["import pandas as pd\n","\n","# Load data\n","df = pd.read_csv(\"/content/drive/MyDrive/BINANCE/TRADE_1H/47991_DataProcessor_v3.csv\")\n","\n","print(df.head(10))\n","# Print the total number of rows and columns\n","num_rows, num_cols = df.shape\n","print(f\"Total Number of Rows: {num_rows}\")\n","print(f\"Total Number of Columns: {num_cols}\")\n","\n","# Display the first few rows of the DataFrame\n","\n","print(df.dtypes)\n","# количество строк и столбцов DataFrame\n","\n","n_rows, n_columns = df.shape\n","print(f\"The DataFrame has {n_rows} rows and {n_columns} columns.\")\n","\n","print(\"Columns in df:\", df.columns)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqzTz1ZC9d1F"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"yjJNgy_a71HM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698921932434,"user_tz":-60,"elapsed":3930,"user":{"displayName":"Oleh Kondracki","userId":"06214181816493942618"}},"outputId":"89efc76e-7aa2-46d9-803e-7af1520cbea8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.3)\n"]}],"source":["!pip install keras xgboost"]},{"cell_type":"code","source":["# 47991\n","\n","import matplotlib.pyplot as plt\n","import logging\n","import pandas as pd\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import time\n","from sklearn.preprocessing import MinMaxScaler\n","import joblib\n","import numpy as np\n","from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n","from sklearn.model_selection import GridSearchCV, cross_val_score, TimeSeriesSplit\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","\n","logging.basicConfig(level=logging.INFO)\n","\n","def log_execution_time(start_time, message=\"Execution time\"):\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    minutes = int(elapsed_time // 60)\n","    seconds = int(elapsed_time % 60)\n","    print(f\"{message}: {minutes} minutes and {seconds} seconds\")\n","\n","class TimeSeriesForecasting:\n","    def __init__(self, data, save_directory):\n","        self.save_directory = save_directory\n","        self.df = data\n","        self.models = {}\n","        self.feature_names = {}\n","        self.tscv = TimeSeriesSplit(n_splits=5)\n","        self.train = pd.DataFrame()\n","        self.val = pd.DataFrame()\n","\n","\n","    def data_quality_check(self):\n","        # Check for missing values\n","        missing_values = self.df.isnull().sum()\n","        columns_with_missing_values = missing_values[missing_values > 0].index.tolist()  # Initialized the missing values columns list\n","\n","        # Fill NaN values in each column with its mean\n","        for column in columns_with_missing_values:\n","            self.df[column].fillna(self.df[column].mean(), inplace=True)\n","        print(\"Missing Values:\\n\", missing_values)\n","\n","        # Check for duplicates\n","        duplicates = self.df.duplicated().sum()\n","        print(\"\\nNumber of duplicate rows:\", duplicates)\n","\n","        # Drop duplicates if any\n","        if duplicates > 0:\n","            self.df.drop_duplicates(inplace=True)\n","            print(\"Dropped duplicates.\")\n","\n","        # Check data types after conversion\n","        data_types_after_conversion = self.df.dtypes\n","        print(\"\\nData Types after conversion:\\n\", data_types_after_conversion)\n","\n","    def generate_future_data(self, timestamps):\n","        future_data = pd.DataFrame({'EVENT_TIME': timestamps})\n","        missing_cols = set(self.train.columns) - set(future_data.columns)\n","        for col in missing_cols:\n","            future_data[col] = np.nan\n","        for col in future_data.columns:\n","            if future_data[col].isnull().any():\n","                future_data[col].fillna(self.train[col].mean(), inplace=True)\n","        if 'EVENT_TIME' in future_data.columns:\n","            future_data['EVENT_TIME'] = pd.to_datetime(future_data['EVENT_TIME'])\n","            future_data.set_index('EVENT_TIME', inplace=True)\n","            for time_feature in ['year', 'month', 'day', 'hour', 'minute', 'second']:\n","                future_data[f'EVENT_TIME_{time_feature}'] = getattr(future_data.index, time_feature)\n","        return future_data\n","\n","    def preprocess_data(self):\n","        print(\"Preprocessing data...\")\n","        if 'EVENT_TIME' in self.df.columns:\n","            self.df['EVENT_TIME'] = pd.to_datetime(self.df['EVENT_TIME'], errors='coerce')\n","            self.df['EVENT_TIME_year'] = self.df['EVENT_TIME'].dt.year\n","            self.df['EVENT_TIME_month'] = self.df['EVENT_TIME'].dt.month\n","            self.df['EVENT_TIME_day'] = self.df['EVENT_TIME'].dt.day\n","            self.df['EVENT_TIME_hour'] = self.df['EVENT_TIME'].dt.hour\n","            self.df['EVENT_TIME_minute'] = self.df['EVENT_TIME'].dt.minute\n","            self.df['EVENT_TIME_second'] = self.df['EVENT_TIME'].dt.second\n","            self.df.drop(columns='EVENT_TIME', inplace=True)  # Drop the 'EVENT_TIME' column\n","\n","        # Convert any string values to numeric\n","        for col in self.df.columns:\n","            self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n","\n","        # Handle non-numeric columns here (if any)\n","\n","        print(\"Data types after preprocessing:\")\n","        print(self.df.dtypes)\n","\n","    def inverse_scale_data(self, data):\n","        scaler = MinMaxScaler()\n","        scaler.fit(self.train)\n","        return scaler.inverse_transform(data)\n","\n","    def _split_data(self, target_col):\n","        train, val = train_test_split(self.df, test_size=0.2, shuffle=False)\n","        print(f\"Training data length: {len(train)}\")\n","        print(f\"Validation data length: {len(val)}\")\n","        return train, val\n","\n","    def train_models(self, target_col, model_types=['GradientBoosting', 'RandomForest', 'XGBoost']):\n","        start_time = time.time()\n","\n","        self.data_quality_check()  # Call data_quality_check method first\n","        self.preprocess_data()  # Then call preprocess_data method\n","\n","        self.train, self.val = self._split_data(target_col)\n","\n","        X_train, y_train = self.train.drop(columns=target_col), self.train[target_col]\n","        X_test, y_test = self.val.drop(columns=target_col), self.val[target_col]\n","\n","        for model_type in model_types:\n","            self._hyperparameter_tuning_and_training(target_col, X_train, y_train, X_test, y_test, model_type)\n","\n","        self.log_execution_time(start_time, \"Total training time\")\n","\n","    def _hyperparameter_tuning_and_training(self, target_col, X_train, y_train, X_test, y_test, model_type='GradientBoosting'):\n","        start_time = time.time()\n","        model, param_grid = self._get_model_and_param_grid(model_type)\n","        grid_search = GridSearchCV(model, param_grid, cv=KFold(n_splits=5), n_jobs=-1)\n","        grid_search.fit(X_train, y_train)\n","        best_model = grid_search.best_estimator_\n","        best_model.fit(X_train, y_train)\n","        self.models[target_col] = best_model\n","        self.feature_names[target_col] = list(X_train.columns)\n","        cv_scores = cross_val_score(best_model, X_test, y_test, cv=KFold(n_splits=5))\n","        print(f'Cross-validation scores for {target_col}: {cv_scores}')\n","        print(f'Mean cross-validation score for {target_col}: {np.mean(cv_scores)}')\n","        importance = best_model.feature_importances_\n","        importance_dict = dict(zip(X_train.columns, importance))\n","        sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n","        print(f\"Top 5 important features for {target_col}: {sorted_importance[:5]}\")\n","        self.log_execution_time(start_time, \"Hyperparameter tuning and training completed in\")\n","\n","    def _get_model_and_param_grid(self, model_type):\n","        if model_type == 'GradientBoosting':\n","            model = GradientBoostingRegressor()\n","            param_grid = {\n","                'n_estimators': [50, 100, 150, 200],\n","                'max_depth': [3, 5, 7, 9],\n","                'learning_rate': [0.001, 0.01, 0.1, 0.5],\n","                'subsample': [0.8, 0.9, 1.0],\n","                'max_features': ['sqrt', 'log2', None]\n","            }\n","        elif model_type == 'RandomForest':\n","            model = RandomForestRegressor()\n","            param_grid = {\n","                'n_estimators': [10, 50, 100, 150],\n","                'max_depth': [3, 5, 7, None],\n","                'max_features': ['sqrt', 'log2', None]\n","            }\n","        elif model_type == 'XGBoost':\n","            model = XGBRegressor()\n","            param_grid = {\n","                'n_estimators': [50, 100, 150, 200],\n","                'max_depth': [3, 5, 7, 9],\n","                'learning_rate': [0.001, 0.01, 0.1, 0.5],\n","                'subsample': [0.8, 0.9, 1.0],\n","                'colsample_bytree': [0.8, 0.9, 1.0],\n","                'gamma': [0, 0.1, 0.2, 0.3]\n","            }\n","        else:\n","            raise ValueError(\"Invalid model_type. Choose 'GradientBoosting', 'RandomForest' or 'XGBoost'\")\n","        return model, param_grid\n","\n","    def predict(self, target_col, X):\n","        model = self.models[target_col]\n","        feature_names = self.feature_names[target_col]\n","        if 'EVENT_TIME' in X.columns:\n","            X = X.drop(columns='EVENT_TIME')\n","        X = X[feature_names]\n","        predictions = model.predict(X)\n","        return predictions\n","\n","    def evaluate_all_models(self, target_col):\n","        X_test, y_test = self.val.drop(columns=target_col), self.val[target_col]\n","        for target_col, model in self.models.items():\n","            y_pred = model.predict(X_test)\n","            self.evaluate_model(y_test, y_pred, target_col)\n","\n","    def evaluate_model(self, y_true, y_pred, model_type):\n","        mse = mean_squared_error(y_true, y_pred)\n","        print(f\"Mean squared error for {model_type}: {mse}\")\n","\n","    def save_model(self, target_col):\n","        model = self.models[target_col]\n","        model_file_path = f\"{self.save_directory}/{target_col}_model.pkl\"\n","        joblib.dump(model, model_file_path)\n","        print(f\"Saved {target_col} model to {model_file_path}\")\n","\n","    def log_execution_time(self, start_time, message):\n","        end_time = time.time()\n","        elapsed_time = end_time - start_time\n","        print(f\"{message}: {elapsed_time} seconds\")\n","\n","    def load_models(self):\n","        for target_col in self.models.keys():\n","            self.models[target_col] = joblib.load(f\"{self.save_directory}/{target_col}_model.pkl\")\n","\n","    def future_predictions(self, target_col, timestamps):\n","        future_data = self.generate_future_data(timestamps)\n","        predictions = self.predict(target_col, future_data)\n","        future_data['predictions'] = predictions\n","        return future_data\n","\n","    def plot_future_predictions(self, target_col, timestamps):\n","        future_data = self.future_predictions(target_col, timestamps)\n","        plt.figure(figsize=(10, 5))\n","        plt.plot(future_data.index, future_data['predictions'])\n","        plt.xlabel('Timestamp')\n","        plt.ylabel(target_col)\n","        plt.title(f'Future Predictions for {target_col}')\n","        plt.show()\n","\n","    def print_memory_usage(self):\n","        total_memory_bytes = self.df.memory_usage(deep=True).sum()\n","        total_memory_megabytes = total_memory_bytes / 1_048_576\n","        print(f\"DataFrame memory usage: {total_memory_megabytes:.2f} MB\")\n","\n","def main():\n","    main_start_time = time.time()\n","    save_directory = \"./models\"  # Update this path as needed\n","    data = pd.read_csv(\"/content/drive/MyDrive/BINANCE/TRADE_1H/47991_DataProcessor_v3.csv\")  # Update this path to your data file\n","    forecasting = TimeSeriesForecasting(data, save_directory=save_directory)\n","\n","    target_col = 'PRICE'  # Use the correct target column name\n","\n","    # Train models (if not already trained)\n","    forecasting.train_models(target_col)\n","\n","    # Check if the model and feature names for the target column are available\n","    if target_col not in forecasting.models:\n","        print(f\"Model for {target_col} is not available.\")\n","    if target_col not in forecasting.feature_names:\n","        print(f\"Feature names for {target_col} are not available.\")\n","\n","    # Get the last timestamp in the dataframe\n","    last_timestamp = forecasting.df['EVENT_TIME'].max()\n","\n","    # Generate a range of timestamps starting from the last timestamp, extending 60 values into the future\n","    timestamps = pd.date_range(start=last_timestamp, periods=61, freq='H')[1:]\n","\n","    # Make future predictions\n","    forecasting.future_predictions(target_col, timestamps)\n","    forecasting.plot_future_predictions(target_col, timestamps)\n","\n","    end_time = time.time()\n","    elapsed_time = end_time - main_start_time\n","    minutes = int(elapsed_time // 60)\n","    seconds = int(elapsed_time % 60)\n","    print(f\"Execution time: {minutes} minutes and {seconds} seconds\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"dQDa5eoNEjd2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fa34a2ba-db31-4f6f-ba82-c596a9511760"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Missing Values:\n"," EVENT_TIME               0\n","TRADE_ID                 0\n","PRICE                    0\n","QUANTITY                 0\n","BUYER_ORDER_ID           0\n","SELLER_ORDER_ID          0\n","IS_BUYER_MARKET_MAKER    0\n","EMA_short                0\n","EMA_long                 0\n","RSI                      0\n","OBV                      0\n","VWAP                     0\n","ATR                      0\n","Parabolic_SAR            0\n","dtype: int64\n","\n","Number of duplicate rows: 0\n","\n","Data Types after conversion:\n"," EVENT_TIME                object\n","TRADE_ID                   int64\n","PRICE                    float64\n","QUANTITY                 float64\n","BUYER_ORDER_ID             int64\n","SELLER_ORDER_ID            int64\n","IS_BUYER_MARKET_MAKER      int64\n","EMA_short                float64\n","EMA_long                 float64\n","RSI                      float64\n","OBV                      float64\n","VWAP                     float64\n","ATR                      float64\n","Parabolic_SAR            float64\n","dtype: object\n","Preprocessing data...\n","Data types after preprocessing:\n","TRADE_ID                   int64\n","PRICE                    float64\n","QUANTITY                 float64\n","BUYER_ORDER_ID             int64\n","SELLER_ORDER_ID            int64\n","IS_BUYER_MARKET_MAKER      int64\n","EMA_short                float64\n","EMA_long                 float64\n","RSI                      float64\n","OBV                      float64\n","VWAP                     float64\n","ATR                      float64\n","Parabolic_SAR            float64\n","EVENT_TIME_year            int64\n","EVENT_TIME_month           int64\n","EVENT_TIME_day             int64\n","EVENT_TIME_hour            int64\n","EVENT_TIME_minute          int64\n","EVENT_TIME_second          int64\n","dtype: object\n","Training data length: 38392\n","Validation data length: 9599\n"]}]},{"cell_type":"code","source":["\n"],"metadata":{"id":"3dudMqjMEja-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0RusO_0a2Vbj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"a2-VF8BqEjYF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"y_FAlx8NEjWQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6TFivHS-EjRm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"N6sb976LEjOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lQVsaZqWEjLI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FLtsHg2UEjIE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Efj0YVN_EjFB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aAxd_CXJEjBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-sxv3IGsEi-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yn154VSTEi7X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EBTbisP5Ei00"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"a340ou4bEivW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xQnUaBosEir_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rDFPcCm8Eio9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Y2drfavoegQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bBKPfol7egOZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Cj2R6_FnegLS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"X9re01m7egFZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NIInagLPegB5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8BO4wMam71EX"},"outputs":[],"source":["# model NeuralNetworkModel\n","\n","import logging\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from sklearn.ensemble import RandomForestRegressor\n","import xgboost as xgb\n","from keras.models import Sequential, load_model\n","from keras.layers import LSTM, Dropout, Dense, BatchNormalization, LeakyReLU\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.regularizers import l2\n","from keras.wrappers.scikit_learn import KerasRegressor\n","from keras_tuner.tuners import RandomSearch\n","import joblib\n","import matplotlib.pyplot as plt\n","import time\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","logging.basicConfig(level=logging.INFO)\n","def log_execution_time(start_time, message=\"Execution time\"):\n","    \"\"\"Utility function to log the execution time.\"\"\"\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    minutes = int(elapsed_time // 60)\n","    seconds = int(elapsed_time % 60)\n","    print(f\"{message}: {minutes} minutes and {seconds} seconds\")\n","\n","class NeuralNetworkModel:\n","\n","    def __init__(self, save_directory, df, X_test, y_test, val):\n","        self.save_directory = save_directory\n","        self.df = df\n","        self.X_test = X_test\n","        self.y_test = y_test\n","        self.val = val\n","        self.predictions = {}\n","        self.future_predictions = {}\n","        self.model = self.build_model()\n","\n","    def build_model(self):\n","        model = Sequential()\n","        model.add(LSTM(100, return_sequences=True, input_shape=(self.X_test.shape[1], self.X_test.shape[2])))\n","        model.add(Dropout(0.2))\n","        model.add(BatchNormalization())\n","\n","        model.add(LSTM(100, return_sequences=True))\n","        model.add(Dropout(0.2))\n","        model.add(BatchNormalization())\n","\n","        model.add(LSTM(50))\n","        model.add(Dropout(0.2))\n","        model.add(BatchNormalization())\n","\n","        model.add(Dense(50, activation='relu', kernel_regularizer=l2(0.01)))\n","        model.add(Dropout(0.2))\n","        model.add(BatchNormalization())\n","\n","        model.add(Dense(50))\n","        model.add(LeakyReLU(alpha=0.01))\n","        model.add(Dropout(0.2))\n","        model.add(BatchNormalization())\n","\n","        model.add(Dense(32, activation='relu'))\n","        model.add(Dropout(0.2))\n","        model.add(BatchNormalization())\n","\n","        model.add(Dense(1))\n","\n","        return model\n","\n","    def fit_model(self, X_train, y_train, X_val, y_val, initial_learning_rate, end_learning_rate, epochs, batch_size):\n","        decay_steps = int(epochs * (len(X_train) / batch_size))\n","        lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n","            initial_learning_rate,\n","            decay_steps=decay_steps,\n","            end_learning_rate=end_learning_rate,\n","            power=1.0\n","        )\n","        optimizer = Adam(learning_rate=lr_schedule)\n","        self.model.compile(optimizer=optimizer, loss='mean_squared_error')\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","        checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n","\n","        callbacks = [early_stopping, checkpoint]\n","\n","        history = self.model.fit(\n","            X_train, y_train,\n","            validation_data=(X_val, y_val),\n","            epochs=epochs,\n","            batch_size=batch_size,\n","            callbacks=callbacks\n","        )\n","\n","        return history\n","\n","    def evaluate_model(self, y_test_original, y_pred_original):\n","        percentage_accuracy = self.calc_percentage_accuracy(y_test_original, y_pred_original)\n","        rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n","        print(f'Root Mean Squared Error: {rmse}')\n","        mae = mean_absolute_error(y_test_original, y_pred_original)\n","        print(f'Mean Absolute Error: {mae}')\n","        self.plot_results(y_test_original, y_pred_original, percentage_accuracy)\n","\n","    def calc_percentage_accuracy(self, y_true, y_pred):\n","        return (1 - np.abs((y_pred - y_true) / y_true)) * 100\n","\n","\n","    def save_future_predictions(self, models, timestamps, filename='Future_Predictions.csv'):\n","        full_path = self.save_directory + filename\n","        future_predictions_df = pd.DataFrame({'EVENT_TIME': timestamps})\n","\n","        for model_name, model in models.items():\n","            column_name = f\"{model_name}_Predicted\"\n","            future_predictions_df[column_name] = self.future_predictions[model_name]\n","\n","        # Save future predictions to CSV\n","        future_predictions_df.to_csv(full_path, index=False)\n","\n","        logging.info(f\"Future predictions saved successfully to {full_path}!\")\n","\n","    def save_all_model_predictions(self, models, filename='Predictions.csv'):\n","        full_path = self.save_directory + filename\n","        predictions_df = self.val.copy()\n","        for model_name in models.keys():\n","            column_name = f\"{model_name}_Predicted\"\n","            predictions_df[column_name] = self.predictions[model_name]\n","            if model_name != \"LSTM\":\n","                predictions_df[column_name] = self.inverse_scale_data(predictions_df[column_name].values.reshape(-1, 1)).flatten()\n","\n","        predictions_df.to_csv(full_path, index=False)\n","        logging.info(f\"Predictions saved successfully to {full_path}!\")\n","\n","    def save_model(self, model, filename):\n","        full_path = self.save_directory + filename\n","        if isinstance(model, (RandomForestRegressor, xgb.XGBRegressor)):\n","            joblib.dump(model, full_path)\n","        elif isinstance(model, Sequential):\n","            model.save(full_path)  # Saving the entire model (architecture + weights)\n","        logging.info(f\"Model saved to {filename}!\")\n","\n","    def load_model(self, filename):\n","        full_path = self.save_directory + filename\n","        if filename.endswith('.h5'):\n","            model = load_model(full_path)  # Loading LSTM model\n","        else:\n","            model = joblib.load(full_path)\n","        logging.info(f\"Model loaded from {filename}!\")\n","        return model\n","\n","\n","    def plot_results(self, y_test_original, y_pred_original, percentage_accuracy):\n","        plt.figure(figsize=(15, 5))\n","        plt.subplot(1, 3, 1)\n","        plt.plot(y_test_original, label='True')\n","        plt.plot(y_pred_original, label='Predicted')\n","        plt.title('Prediction vs Real - CLOSE_PRICE')\n","        plt.xlabel('Observation')\n","        plt.ylabel('CLOSE_PRICE')\n","        plt.legend()\n","\n","        plt.subplot(1, 3, 3)\n","        plt.plot(percentage_accuracy, label='Percentage Accuracy')\n","        plt.title('Percentage Accuracy')\n","        plt.xlabel('Observation')\n","        plt.ylabel('Accuracy (%)')\n","        plt.legend()\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    def main():\n","        main_start_time = time.time()\n","\n","        save_path = \"/content/drive/MyDrive/BINANCE/TRADE_1H/\"\n","        forecasting = TimeSeriesForecasting(\"/content/drive/MyDrive/BINANCE/TRADE_1H/47991_Scaled_Data_v3.csv\", save_directory=save_path)\n","\n","        # Assuming you have a processor object defined elsewhere in your program\n","        processor = Processor()  # or however you create/obtain this object\n","\n","        # Print memory usage after reading the data\n","        print(\"Memory usage after reading data:\")\n","        processor.print_memory_usage()\n","\n","        # Print memory usage after scaling\n","        print(\"\\nMemory usage after scaling:\")\n","        processor.print_memory_usage()\n","\n","        # Assuming you've defined or will define these methods in the TimeSeriesForecasting class:\n","        forecasting.save_all_model_predictions(models, 'All_Model_Predictions.csv')\n","\n","        # Assuming lstm_model is an attribute of the forecasting object\n","        forecasting.lstm_model.save('neural_network_model.keras')\n","\n","        # Save the dataframe\n","        processor.save_dataframe()\n","\n","        print(processor.df.head())\n","        print(f\"The DataFrame has {processor.df.shape[0]} rows and {processor.df.shape[1]} columns.\")\n","\n","        end_time = time.time()  # End timing\n","        elapsed_time = end_time - main_start_time\n","        minutes = int(elapsed_time // 60)\n","        seconds = int(elapsed_time % 60)\n","\n","        print(f\"Execution time: {minutes} minutes and {seconds} seconds\")\n","\n","    # Assuming you have a log_execution_time function defined elsewhere in your program\n","    # log_execution_time(main_start_time)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tArRlvTQ706p"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WYKb0TP703a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KMMEweY700O"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":32,"metadata":{"id":"X8lEnGrCOClc","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1698930641617,"user_tz":-60,"elapsed":42068,"user":{"displayName":"Oleh Kondracki","userId":"06214181816493942618"}},"outputId":"946f591b-5af3-4052-ab3c-03261be15a54"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training data length: 38392\n","Validation data length: 9599\n","Preprocessing data...\n","Data types after preprocessing:\n","TRADE_ID                   int64\n","PRICE                    float64\n","QUANTITY                 float64\n","BUYER_ORDER_ID             int64\n","SELLER_ORDER_ID            int64\n","IS_BUYER_MARKET_MAKER      int64\n","EMA_short                float64\n","EMA_long                 float64\n","RSI                      float64\n","OBV                      float64\n","VWAP                     float64\n","ATR                      float64\n","Parabolic_SAR            float64\n","EVENT_TIME_year            int64\n","EVENT_TIME_month           int64\n","EVENT_TIME_day             int64\n","EVENT_TIME_hour            int64\n","EVENT_TIME_minute          int64\n","EVENT_TIME_second          int64\n","dtype: object\n","Missing Values:\n"," TRADE_ID                 0\n","PRICE                    0\n","QUANTITY                 0\n","BUYER_ORDER_ID           0\n","SELLER_ORDER_ID          0\n","IS_BUYER_MARKET_MAKER    0\n","EMA_short                0\n","EMA_long                 0\n","RSI                      0\n","OBV                      0\n","VWAP                     0\n","ATR                      0\n","Parabolic_SAR            0\n","EVENT_TIME_year          0\n","EVENT_TIME_month         0\n","EVENT_TIME_day           0\n","EVENT_TIME_hour          0\n","EVENT_TIME_minute        0\n","EVENT_TIME_second        0\n","dtype: int64\n","\n","Number of duplicate rows: 0\n","\n","Data Types after conversion:\n"," TRADE_ID                   int32\n","PRICE                    float32\n","QUANTITY                 float32\n","BUYER_ORDER_ID             int32\n","SELLER_ORDER_ID            int32\n","IS_BUYER_MARKET_MAKER      int32\n","EMA_short                float32\n","EMA_long                 float32\n","RSI                      float32\n","OBV                      float32\n","VWAP                     float32\n","ATR                      float32\n","Parabolic_SAR            float32\n","EVENT_TIME_year            int32\n","EVENT_TIME_month           int32\n","EVENT_TIME_day             int32\n","EVENT_TIME_hour            int32\n","EVENT_TIME_minute          int32\n","EVENT_TIME_second          int32\n","dtype: object\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-8d77219ba9f0>\u001b[0m in \u001b[0;36m<cell line: 278>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-32-8d77219ba9f0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m# Train models (if not already trained)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mforecasting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;31m# Check if the model and feature names for the target column are available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-8d77219ba9f0>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(self, target_col, model_types)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hyperparameter_tuning_and_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_execution_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Total training time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-8d77219ba9f0>\u001b[0m in \u001b[0;36m_hyperparameter_tuning_and_training\u001b[0;34m(self, target_col, X_train, y_train, X_test, y_test, model_type)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model_and_param_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: \nAll the 2880 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n576 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\", line 429, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\", line 2070, in __array__\n    return np.asarray(self._values, dtype=dtype)\nValueError: could not convert string to float: '2023-10-10 11:49:41'\n\n--------------------------------------------------------------------------------\n2304 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\", line 429, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\", line 2070, in __array__\n    return np.asarray(self._values, dtype=dtype)\nValueError: could not convert string to float: '2023-10-10 11:37:37'\n"]}],"source":["# model ML\n","import matplotlib.pyplot as plt\n","import logging\n","import pandas as pd\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import time\n","from sklearn.preprocessing import MinMaxScaler\n","import joblib\n","import numpy as np\n","from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n","from sklearn.model_selection import GridSearchCV, cross_val_score, TimeSeriesSplit\n","#from neural_network_model import NeuralNetworkModel  # Assuming NeuralNetworkModel is defined in another file\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","\n","\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n","#data_path = \"/content/drive/MyDrive/BINANCE/TRADE_1H/47991_DataProcessor_v3.csv\"  # Update the path to your file\n","\n","logging.basicConfig(level=logging.INFO)\n","def log_execution_time(start_time, message=\"Execution time\"):\n","    \"\"\"Utility function to log the execution time.\"\"\"\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    minutes = int(elapsed_time // 60)\n","    seconds = int(elapsed_time % 60)\n","    print(f\"{message}: {minutes} minutes and {seconds} seconds\")\n","\n","class TimeSeriesForecasting:\n","    def __init__(self, data_path, save_directory):\n","        self.save_directory = save_directory\n","        self.df = pd.read_csv(data_path)\n","        self.models = {}\n","        self.feature_names = {}\n","        self.tscv = TimeSeriesSplit(n_splits=5)\n","        self.train = pd.DataFrame()\n","        self.val = pd.DataFrame()\n","\n","    def data_quality_check(self):\n","        # Check for missing values\n","        missing_values = self.df.isnull().sum()\n","        columns_with_missing_values = missing_values[missing_values > 0].index.tolist()  # Initialized the missing values columns list\n","\n","        # Fill NaN values in each column with its mean\n","        for column in columns_with_missing_values:\n","            self.df[column].fillna(self.df[column].mean(), inplace=True)\n","        print(\"Missing Values:\\n\", missing_values)\n","\n","        # Check for duplicates\n","        duplicates = self.df.duplicated().sum()\n","        print(\"\\nNumber of duplicate rows:\", duplicates)\n","\n","        # Drop duplicates if any\n","        if duplicates > 0:\n","            self.df.drop_duplicates(inplace=True)\n","            print(\"Dropped duplicates.\")\n","\n","        # Convert int64 columns to int32\n","        for col in self.df.select_dtypes(include=['int64']).columns:\n","            self.df[col] = self.df[col].astype('int32')\n","\n","        # Convert float64 columns to float32\n","        for col in self.df.select_dtypes(include=['float64']).columns:\n","            self.df[col] = self.df[col].astype('float32')\n","\n","        # Check data types after conversion\n","        data_types_after_conversion = self.df.dtypes\n","        print(\"\\nData Types after conversion:\\n\", data_types_after_conversion)\n","\n","    def preprocess_data(self):\n","        print(\"Preprocessing data...\")\n","        # Check if 'timestamp' column exists\n","        if 'timestamp' in self.df.columns:\n","            # Convert timestamp to datetime object\n","            self.df['timestamp'] = pd.to_datetime(self.df['timestamp'])\n","\n","            # Extract numerical features from datetime object\n","            self.df['year'] = self.df['timestamp'].dt.year\n","            self.df['month'] = self.df['timestamp'].dt.month\n","            self.df['day'] = self.df['timestamp'].dt.day\n","            self.df['hour'] = self.df['timestamp'].dt.hour\n","            self.df['minute'] = self.df['timestamp'].dt.minute\n","            self.df['second'] = self.df['timestamp'].dt.second\n","\n","            # Drop the original timestamp column\n","            self.df.drop(columns=['timestamp'], inplace=True)\n","\n","        if 'EVENT_TIME' in self.df.columns:\n","            # Convert EVENT_TIME to datetime object\n","            self.df['EVENT_TIME'] = pd.to_datetime(self.df['EVENT_TIME'], errors='coerce')\n","\n","            # Extract numerical features from datetime object\n","            self.df['EVENT_TIME_year'] = self.df['EVENT_TIME'].dt.year\n","            self.df['EVENT_TIME_month'] = self.df['EVENT_TIME'].dt.month\n","            self.df['EVENT_TIME_day'] = self.df['EVENT_TIME'].dt.day\n","            self.df['EVENT_TIME_hour'] = self.df['EVENT_TIME'].dt.hour\n","            self.df['EVENT_TIME_minute'] = self.df['EVENT_TIME'].dt.minute\n","            self.df['EVENT_TIME_second'] = self.df['EVENT_TIME'].dt.second\n","\n","            # Drop the original EVENT_TIME column\n","            self.df.drop(columns=['EVENT_TIME'], inplace=True)\n","\n","        print(\"Data types after preprocessing:\")\n","        print(self.df.dtypes)\n","\n","    def inverse_scale_data(self, data):\n","        scaler = MinMaxScaler()\n","        scaler.fit(self.train)\n","        return scaler.inverse_transform(data)\n","\n","    def generate_future_data(self, timestamps):\n","        future_data = pd.DataFrame({'EVENT_TIME': timestamps})\n","        missing_cols = set(self.train.columns) - set(future_data.columns)\n","        for col in missing_cols:\n","            future_data[col] = np.nan\n","        for col in future_data.columns:\n","            if future_data[col].isnull().any():\n","                future_data[col].fillna(self.train[col].mean(), inplace=True)\n","        if 'EVENT_TIME' in future_data.columns:\n","            future_data['EVENT_TIME'] = pd.to_datetime(future_data['EVENT_TIME'])\n","            future_data.set_index('EVENT_TIME', inplace=True)\n","            for time_feature in ['YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND']:\n","                future_data[time_feature] = getattr(future_data.index, time_feature.lower())\n","        return future_data\n","\n","\n","    def _split_data(self, target_col):\n","        train, val = train_test_split(self.df, test_size=0.2, shuffle=False)\n","        print(f\"Training data length: {len(train)}\")\n","        print(f\"Validation data length: {len(val)}\")\n","        return train, val\n","\n","\n","    def train_models(self, target_col, model_types=['GradientBoosting', 'RandomForest', 'XGBoost']):\n","        start_time = time.time()\n","        self.train, self.val = self._split_data(target_col)\n","        self.preprocess_data()\n","        self.data_quality_check()\n","        X_train, y_train = self.train.drop(columns=target_col), self.train[target_col]\n","        X_test, y_test = self.val.drop(columns=target_col), self.val[target_col]\n","        for model_type in model_types:\n","            self._hyperparameter_tuning_and_training(target_col, X_train, y_train, X_test, y_test, model_type)\n","        self.log_execution_time(start_time, \"Total training time\")\n","\n","\n","    def _hyperparameter_tuning_and_training(self, target_col, X_train, y_train, X_test, y_test, model_type='GradientBoosting'):\n","        start_time = time.time()\n","        model, param_grid = self._get_model_and_param_grid(model_type)\n","        grid_search = GridSearchCV(model, param_grid, cv=KFold(n_splits=5), n_jobs=-1)\n","        grid_search.fit(X_train, y_train)\n","        best_model = grid_search.best_estimator_\n","        best_model.fit(X_train, y_train)\n","        self.models[target_col] = best_model\n","        self.feature_names[target_col] = list(X_train.columns)\n","        cv_scores = cross_val_score(best_model, X_test, y_test, cv=KFold(n_splits=5))\n","        print(f'Cross-validation scores for {target_col}: {cv_scores}')\n","        print(f'Mean cross-validation score for {target_col}: {np.mean(cv_scores)}')\n","        importance = best_model.feature_importances_\n","        importance_dict = dict(zip(X_train.columns, importance))\n","        sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n","        print(f\"Top 5 important features for {target_col}: {sorted_importance[:5]}\")\n","        self.log_execution_time(start_time, \"Hyperparameter tuning and training completed in\")\n","\n","    def _get_model_and_param_grid(self, model_type):\n","        if model_type == 'GradientBoosting':\n","            model = GradientBoostingRegressor()\n","            param_grid = {\n","                'n_estimators': [50, 100, 150, 200],\n","                'max_depth': [3, 5, 7, 9],\n","                'learning_rate': [0.001, 0.01, 0.1, 0.5],\n","                'subsample': [0.8, 0.9, 1.0],\n","                'max_features': ['sqrt', 'log2', None]\n","            }\n","        elif model_type == 'RandomForest':\n","            model = RandomForestRegressor()\n","            param_grid = {\n","                'n_estimators': [10, 50, 100, 150],\n","                'max_depth': [3, 5, 7, None],\n","                'max_features': ['sqrt', 'log2', None]\n","            }\n","        elif model_type == 'XGBoost':\n","            model = XGBRegressor()\n","            param_grid = {\n","                'n_estimators': [50, 100, 150, 200],\n","                'max_depth': [3, 5, 7, 9],\n","                'learning_rate': [0.001, 0.01, 0.1, 0.5],\n","                'subsample': [0.8, 0.9, 1.0],\n","                'colsample_bytree': [0.8, 0.9, 1.0],\n","                'gamma': [0, 0.1, 0.2, 0.3]\n","            }\n","        else:\n","            raise ValueError(\"Invalid model_type. Choose 'GradientBoosting', 'RandomForest' or 'XGBoost'\")\n","        return model, param_grid\n","\n","    def predict(self, target_col, X):\n","        model = self.models[target_col]\n","        feature_names = self.feature_names[target_col]\n","        X = X[feature_names]\n","        predictions = model.predict(X)\n","        return predictions\n","\n","    def evaluate_all_models(self, target_col):\n","        X_test, y_test = self.val.drop(columns=target_col), self.val[target_col]\n","        for target_col, model in self.models.items():\n","            y_pred = model.predict(X_test)\n","            self.evaluate_model(y_test, y_pred, target_col)\n","\n","    def evaluate_model(self, y_true, y_pred, model_type):\n","        mse = mean_squared_error(y_true, y_pred)\n","        print(f\"Mean squared error for {model_type}: {mse}\")\n","\n","    def save_model(self, target_col):\n","        model = self.models[target_col]\n","        model_file_path = f\"{self.save_directory}/{target_col}_model.pkl\"\n","        joblib.dump(model, model_file_path)\n","        print(f\"Saved {target_col} model to {model_file_path}\")\n","\n","    def log_execution_time(self, start_time, message):\n","        end_time = time.time()\n","        elapsed_time = end_time - start_time\n","        print(f\"{message}: {elapsed_time} seconds\")\n","\n","    def load_models(self):\n","        for target_col in self.models.keys():\n","            self.models[target_col] = joblib.load(f\"{self.save_directory}/{target_col}_model.pkl\")\n","\n","    def future_predictions(self, target_col, timestamps):\n","        future_data = self.generate_future_data(timestamps)\n","        predictions = self.predict(target_col, future_data)\n","        future_data['predictions'] = predictions\n","        return future_data\n","\n","    def plot_future_predictions(self, target_col):\n","        future_data = self.future_predictions(target_col, timestamps)\n","        plt.figure(figsize=(10, 5))\n","        plt.plot(future_data.index, future_data['predictions'])\n","        plt.xlabel('Timestamp')\n","        plt.ylabel(target_col)\n","        plt.title(f'Future Predictions for {target_col}')\n","        plt.show()\n","\n","    def print_memory_usage(self):\n","        total_memory_bytes = self.df.memory_usage(deep=True).sum()\n","        total_memory_megabytes = total_memory_bytes / 1_048_576\n","        print(f\"DataFrame memory usage: {total_memory_megabytes:.2f} MB\")\n","\n","def main():\n","    main_start_time = time.time()\n","    save_path = \"/content/drive/MyDrive/BINANCE/TRADE_1H/\"\n","    forecasting = TimeSeriesForecasting(\"/content/drive/MyDrive/BINANCE/TRADE_1H/47991_DataProcessor_v3.csv\", save_directory=save_path)\n","\n","    timestamps = pd.date_range(start='2023-10-10', end='2023-10-11', freq='H')\n","    target_col = 'PRICE'  # Use the correct target column name\n","\n","    # Train models (if not already trained)\n","    forecasting.train_models(target_col)\n","\n","    # Check if the model and feature names for the target column are available\n","    if target_col not in forecasting.models:\n","        print(f\"Model for {target_col} is not available.\")\n","    if target_col not in forecasting.feature_names:\n","        print(f\"Feature names for {target_col} are not available.\")\n","\n","    # Make future predictions\n","    forecasting.future_predictions(target_col, timestamps)\n","    forecasting.plot_future_predictions(target_col)\n","\n","    end_time = time.time()\n","    elapsed_time = end_time - main_start_time\n","    minutes = int(elapsed_time // 60)\n","    seconds = int(elapsed_time % 60)\n","    print(f\"Execution time: {minutes} minutes and {seconds} seconds\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8ievAb370wu"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"OsEmINardg-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_VsnKpL70jJ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2DM27sM70gb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vO1Lk_ah70do"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjmkQ21w70bA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eiBVoR8U70YA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmMHLibq70VK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mk3GpOy470R3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3RaLiB470PA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1blP_47g70K9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3g73f18xzFnW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4e5jGo6DzFk0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cC-hd-GfzFiJ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9de0oXdTzFfK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOdG6jcvzFb6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIpwDd7WzFY8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3rWZ0W2zFWF"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1B85xDD5OTNKj6B8urulfkTV9gGzyizMm","timestamp":1697645940867}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}