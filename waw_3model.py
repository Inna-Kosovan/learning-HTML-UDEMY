# -*- coding: utf-8 -*-
"""waw_3model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1140imro6yEShQIrFqr-wAaEUKue9-AGz
"""

import io
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from google.colab import files
from xgboost import XGBRegressor
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import SelectFromModel

# Upload the data file
uploaded = files.upload()
print(uploaded.keys())

# Read data
df = pd.read_csv(io.BytesIO(uploaded['BTCUSDT_2023_DATA_TABLE.csv']))

pip install xgboost



# Drop columns if they exist
df.drop(columns=['HOUR', 'MINUTE'], errors='ignore', inplace=True)

# Feature Engineering
if 'TIMESTAMP' in df.columns:
    df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])
    df.set_index('TIMESTAMP', inplace=True)
    df['YEAR'] = df.index.year
    df['MONTH'] = df.index.month
    df['DAY'] = df.index.day
    df['WEEKDAY'] = df.index.weekday

# Handling missing values
df.fillna(df.mean(), inplace=True)

# Defining target and features
target = 'VOLUME'
if target in df.columns:
    X = df.drop(columns=[target])
    y = df[target]
else:
    print(f"'{target}' column not found")
    # Handle this case as needed

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Defining pipeline with scaler, feature transformation (Polynomial Features) and model
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('poly_features', PolynomialFeatures(degree=2)),
    ('model', XGBRegressor(random_state=42))
])


# Hyperparameter grid
param_grid = {
    'model__n_estimators': [50, 100, 150],
    'model__max_depth': [4, 5, 6],
    'model__learning_rate': [0.01, 0.1, 0.2],
    'poly_features__degree': [2, 3]
}

# Grid search with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)


# Predictions
y_pred = grid_search.best_estimator_.predict(X_test)

# Evaluation
print(f'Mean Absolute Error: {mean_absolute_error(y_test, y_pred)}')
print(f'Root Mean Squared Error: {mean_squared_error(y_test, y_pred, squared=False)}')
print(f'R² Score: {r2_score(y_test, y_pred)}')

# Feature Importance & Visualization (modify as necessary based on the best model from Grid Search)
# feature_importances = grid_search.best_estimator_.named_steps['model'].feature_importances_
# print('Feature Importances:', dict(zip(X_train.columns, feature_importances)))

# Residuals
residuals = y_test - y_pred
# ... (the earlier part of your script)

# Predictions
y_pred = grid_search.best_estimator_.predict(X_test)

# Evaluation
print(f'Mean Absolute Error: {mean_absolute_error(y_test, y_pred)}')
print(f'Root Mean Squared Error: {mean_squared_error(y_test, y_pred, squared=False)}')
print(f'R² Score: {r2_score(y_test, y_pred)}')

# Visualization
plt.figure(figsize=(12, 5))

# Subplot 1: Bar plot showing the spectrum of real and predicted values
plt.subplot(1, 2, 1)
plt.bar(range(len(y_test)), y_test, alpha=0.7, label='Real Data', color='b')
plt.bar(range(len(y_pred)), y_pred, alpha=0.7, label='Predictions', color='r')
plt.xlabel('Index')
plt.ylabel('Values')
plt.title('Spectrum of Real Data and Predictions (Bar Plot)')
plt.legend()

# Subplot 2: Scatter plot showing the spectrum of real and predicted values
plt.subplot(1, 2, 2)
plt.scatter(range(len(y_test)), y_test, alpha=0.7, label='Real Data', color='b')
plt.scatter(range(len(y_pred)), y_pred, alpha=0.7, label='Predictions', color='r')
plt.xlabel('Index')
plt.ylabel('Values')
plt.title('Spectrum of Real Data and Predictions (Scatter Plot)')
plt.legend()

plt.tight_layout()
plt.show()

#Visualization
plt.figure(figsize=(12, 5))

#plt.subplot(1, 2, 1)
#sns.scatterplot(x=y_pred, y=residuals)
#plt.axhline(y=0, color='r', linestyle='--')
#plt.title('Residuals vs Predicted')
#plt.xlabel('Predicted Values')
#plt.ylabel('Residuals')

#plt.subplot(1, 2, 2)
#sns.histplot(residuals, kde=True)
#plt.title('Distribution of Residuals')
#plt.xlabel('Residuals')

#plt.tight_layout()
#plt.show()





!pip install pandas pandas_ta sklearn textblob numpy

!pip install pandas_ta

!pip install pandas pandas_ta sklearn numpy

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.feature_selection import RFE
import numpy as np
import pandas_ta as ta
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
from google.colab import files
import io

# Upload the data file
uploaded = files.upload()
print(uploaded.keys())  # This will print the names of uploaded files

# Then you can adjust the following line with the correct file name
df = pd.read_csv(io.BytesIO(uploaded['BTCUSDT_2023_DATA_TABLE (3).csv']))

# Convert 'TIMESTAMP' column to datetime and create time features
if 'TIMESTAMP' in df.columns:
    df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])
    df.set_index('TIMESTAMP', inplace=True)
    df['YEAR'] = df.index.year
    df['MONTH'] = df.index.month
    df['DAY'] = df.index.day
    df['HOUR'] = df.index.hour
    df['MINUTE'] = df.index.minute
    df['WEEKDAY'] = df.index.weekday

# Existing features
df['AVG_PRICE'] = (df['HIGH_PRICE'] + df['LOW_PRICE'] + df['CLOSE_PRICE'] + df['OPEN_PRICE']) / 4
df['PRICE_RANGE'] = df['HIGH_PRICE'] - df['LOW_PRICE']
df['DAY_VOLATILITY'] = (df['HIGH_PRICE'] - df['LOW_PRICE']) / df['LOW_PRICE']
df['LAGGED_VOLUME_1'] = df['VOLUME'].shift(1)
df['MOMENTUM'] = df['CLOSE_PRICE'] - df['CLOSE_PRICE'].shift(4)

# Technical Analysis features using pandas_ta
df['MACD'] = ta.macd(df['CLOSE_PRICE'])['MACD_12_26_9']
df['MACD_HIST'] = ta.macd(df['CLOSE_PRICE'])['MACDh_12_26_9']
df['MACD_SIGNAL'] = ta.macd(df['CLOSE_PRICE'])['MACDs_12_26_9']

bollinger_bands = ta.bbands(df['CLOSE_PRICE'])
df['BOLL_UPPER'] = bollinger_bands['BBU_5_2.0']
df['BOLL_MID'] = bollinger_bands['BBM_5_2.0']
df['BOLL_LOWER'] = bollinger_bands['BBL_5_2.0']

# More technical indicators
df['EMA'] = ta.ema(df['CLOSE_PRICE'], length=14)
df['WMA'] = ta.wma(df['CLOSE_PRICE'], length=14)
df['RSI'] = ta.rsi(df['CLOSE_PRICE'], length=14)
df['ATR'] = ta.atr(df['HIGH_PRICE'], df['LOW_PRICE'], df['CLOSE_PRICE'], length=14)
df['STOCH_OSC'] = 100 * (df['CLOSE_PRICE'] - df['LOW_PRICE'].rolling(window=14).min()) / (df['HIGH_PRICE'].rolling(window=14).max() - df['LOW_PRICE'].rolling(window=14).min())

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler

# Statistical features
window = 14
df['ROLLING_MEAN'] = df['CLOSE_PRICE'].rolling(window=window).mean()
df['ROLLING_STD'] = df['CLOSE_PRICE'].rolling(window=window).std()

# Calendar Features
df['DAY_OF_WEEK'] = df.index.dayofweek
df['IS_WEEKEND'] = df['DAY_OF_WEEK'].apply(lambda x: 1 if x >= 5 else 0)

# Handling missing values
df = df.dropna()

# Selecting target and features
target = 'VOLUME'
features = df.drop(columns=[target])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Normalizing the features (optional but can sometimes improve performance)
scaler = StandardScaler()
scaled_df = scaler.fit_transform(df.drop(columns=[target]))
scaled_df = pd.DataFrame(scaled_df, columns=df.drop(columns=[target]).columns, index=df.index)

best_model = GradientBoostingRegressor(random_state=42)
grid_search = GridSearchCV(best_model, param_grid, cv=3)
grid_search.fit(X_train, y_train)

# Feature Importance
importance = grid_search.best_estimator_.feature_importances_
importance_dict = dict(zip(X_train.columns, importance))
print('Feature Importances:', importance_dict)

# Histogrambest_model
num_bins = 30
data_range = [df['VOLUME'].quantile(0.05), df['VOLUME'].quantile(0.95)]

plt.figure(figsize=(10,6))
plt.hist(df['VOLUME'], bins=num_bins, range=data_range, edgecolor='k', alpha=0.7)
plt.xlabel('Volume')
plt.ylabel('Frequency')
plt.title('Volume Distribution (Filtered)')
plt.tight_layout()
plt.show()

# Making predictions
y_pred = grid_search.best_estimator_.predict(X_test)

# Creating a dataframe to compare real and predicted values
comparison_df = pd.DataFrame({'Real Volume': y_test, 'Predicted Volume': y_pred})

# Calculating error metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)

print(f'Mean Absolute Error: {mae}')
print(f'Root Mean Squared Error: {rmse}')
print(comparison_df)

import matplotlib.pyplot as plt
import pandas as pd

# ... (same data preparation as above)

# Plotting
plt.figure(figsize=(18,14))
plt.plot(df.index, df['Actual'], label='Actual', marker='o', markersize=4)
plt.plot(df.index, df['Predicted'], label='Predicted', marker='x', markersize=4)

# Adding annotations
for i, row in df.iterrows():
    plt.text(i, row['Actual'], f'{row["Actual"]:.2f}', fontsize=8, va='bottom', ha='right')
    plt.text(i, row['Predicted'], f'{row["Predicted"]:.2f}', fontsize=8, va='top', ha='left')

plt.title('Actual vs Predicted Values')
plt.xlabel('Timestamp')
plt.ylabel('Value')
plt.grid(True)
plt.legend()
plt.show()







!pip install pandas pandas_ta sklearn textblob numpy

!pip install pandas_ta

!pip install pandas pandas_ta sklearn numpy

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.feature_selection import RFE
import numpy as np
import pandas_ta as ta
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
from google.colab import files
import io

# Upload the data file
uploaded = files.upload()
print(uploaded.keys())  # This will print the names of uploaded files

# Then you can adjust the following line with the correct file name
df = pd.read_csv(io.BytesIO(uploaded['BTCUSDT_2023_DATA_TABLE (3).csv']))

# Convert 'TIMESTAMP' column to datetime and create time features
if 'TIMESTAMP' in df.columns:
    df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])
    df.set_index('TIMESTAMP', inplace=True)
    df['YEAR'] = df.index.year
    df['MONTH'] = df.index.month
    df['DAY'] = df.index.day
    df['HOUR'] = df.index.hour
    df['MINUTE'] = df.index.minute
    df['WEEKDAY'] = df.index.weekday

# Existing features
df['AVG_PRICE'] = (df['HIGH_PRICE'] + df['LOW_PRICE'] + df['CLOSE_PRICE'] + df['OPEN_PRICE']) / 4
df['PRICE_RANGE'] = df['HIGH_PRICE'] - df['LOW_PRICE']
df['DAY_VOLATILITY'] = (df['HIGH_PRICE'] - df['LOW_PRICE']) / df['LOW_PRICE']
df['LAGGED_VOLUME_1'] = df['VOLUME'].shift(1)
df['MOMENTUM'] = df['CLOSE_PRICE'] - df['CLOSE_PRICE'].shift(4)

# Technical Analysis features using pandas_ta
df['MACD'] = ta.macd(df['CLOSE_PRICE'])['MACD_12_26_9']
df['MACD_HIST'] = ta.macd(df['CLOSE_PRICE'])['MACDh_12_26_9']
df['MACD_SIGNAL'] = ta.macd(df['CLOSE_PRICE'])['MACDs_12_26_9']

bollinger_bands = ta.bbands(df['CLOSE_PRICE'])
df['BOLL_UPPER'] = bollinger_bands['BBU_5_2.0']
df['BOLL_MID'] = bollinger_bands['BBM_5_2.0']
df['BOLL_LOWER'] = bollinger_bands['BBL_5_2.0']

# More technical indicators
df['EMA'] = ta.ema(df['CLOSE_PRICE'], length=14)
df['WMA'] = ta.wma(df['CLOSE_PRICE'], length=14)
df['RSI'] = ta.rsi(df['CLOSE_PRICE'], length=14)
df['ATR'] = ta.atr(df['HIGH_PRICE'], df['LOW_PRICE'], df['CLOSE_PRICE'], length=14)
df['STOCH_OSC'] = 100 * (df['CLOSE_PRICE'] - df['LOW_PRICE'].rolling(window=14).min()) / (df['HIGH_PRICE'].rolling(window=14).max() - df['LOW_PRICE'].rolling(window=14).min())

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler

# Statistical features
window = 14
df['ROLLING_MEAN'] = df['CLOSE_PRICE'].rolling(window=window).mean()
df['ROLLING_STD'] = df['CLOSE_PRICE'].rolling(window=window).std()

# Calendar Features
df['DAY_OF_WEEK'] = df.index.dayofweek
df['IS_WEEKEND'] = df['DAY_OF_WEEK'].apply(lambda x: 1 if x >= 5 else 0)

# Handling missing values
df = df.dropna()

# Selecting target and features
target = 'VOLUME'
features = df.drop(columns=[target])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Normalizing the features (optional but can sometimes improve performance)
scaler = StandardScaler()
scaled_df = scaler.fit_transform(df.drop(columns=[target]))
scaled_df = pd.DataFrame(scaled_df, columns=df.drop(columns=[target]).columns, index=df.index)

best_model = GradientBoostingRegressor(random_state=42)
grid_search = GridSearchCV(best_model, param_grid, cv=3)
grid_search.fit(X_train, y_train)

# Feature Importance
importance = grid_search.best_estimator_.feature_importances_
importance_dict = dict(zip(X_train.columns, importance))
print('Feature Importances:', importance_dict)

# Histogrambest_model
num_bins = 30
data_range = [df['VOLUME'].quantile(0.05), df['VOLUME'].quantile(0.95)]

plt.figure(figsize=(10,6))
plt.hist(df['VOLUME'], bins=num_bins, range=data_range, edgecolor='k', alpha=0.7)
plt.xlabel('Volume')
plt.ylabel('Frequency')
plt.title('Volume Distribution (Filtered)')
plt.tight_layout()
plt.show()

# Making predictions
y_pred = grid_search.best_estimator_.predict(X_test)

# Creating a dataframe to compare real and predicted values
comparison_df = pd.DataFrame({'Real Volume': y_test, 'Predicted Volume': y_pred})

# Calculating error metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)

print(f'Mean Absolute Error: {mae}')
print(f'Root Mean Squared Error: {rmse}')
print(comparison_df)

import matplotlib.pyplot as plt
import pandas as pd

# ... (same data preparation as above)

# Plotting
plt.figure(figsize=(18,14))
plt.plot(df.index, df['Actual'], label='Actual', marker='o', markersize=4)
plt.plot(df.index, df['Predicted'], label='Predicted', marker='x', markersize=4)

# Adding annotations
for i, row in df.iterrows():
    plt.text(i, row['Actual'], f'{row["Actual"]:.2f}', fontsize=8, va='bottom', ha='right')
    plt.text(i, row['Predicted'], f'{row["Predicted"]:.2f}', fontsize=8, va='top', ha='left')

plt.title('Actual vs Predicted Values')
plt.xlabel('Timestamp')
plt.ylabel('Value')
plt.grid(True)
plt.legend()
plt.show()

import io
import numpy as np
import pandas as pd
import pandas_ta as ta
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
from google.colab import files
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
import seaborn as sns
from sklearn.metrics import mean_squared_error

# Upload the data file
uploaded = files.upload()
print(uploaded.keys())  # This will print the names of uploaded files

df = pd.read_csv(io.BytesIO(uploaded['BTCUSDT_2023_DATA_TABLE (5).csv']))

if 'HOUR' in df.columns and 'MINUTE' in df.columns:
    df = df.drop(columns=['HOUR', 'MINUTE'])
else:
    print("Columns 'HOUR' and/or 'MINUTE' not found")

!pip install statsmodels

# Step 3: Feature Engineering
if 'TIMESTAMP' in df.columns:
    df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])
    df.set_index('TIMESTAMP', inplace=True)
    df['YEAR'] = df.index.year
    df['MONTH'] = df.index.month
    df['DAY'] = df.index.day
    df['HOUR'] = df.index.hour
    df['MINUTE'] = df.index.minute
    df['WEEKDAY'] = df.index.weekday


# Step 5: Model Definition and Training

scaler = StandardScaler()

target = 'VOLUME'
if target in df.columns:
    X = df.drop(columns=[target])  # Features
    y = df[target]  # Target
else:
    print(f"'{target}' column not found")
    # You might want to set X and y to some default or handle this case in some way

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Scale the data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Now, you can proceed to use X and y with your model
model = RandomForestRegressor()
model.fit(X, y)


# Get feature importance
feature_importances = model.feature_importances_

# Assuming features is a list of feature names
for feature, importance in zip(features, feature_importances):
    print(f"Feature: {feature}, Importance: {importance}")


# Model Definition, Fitting, and Evaluation
param_grid = {
    'n_estimators': [10, 50, 120],
    'max_depth': [3, 4, 5, 6, 7],
    'learning_rate': [0.01, 0.1, 0.5, 0.09]
}

best_model = GradientBoostingRegressor(random_state=42)
grid_search = GridSearchCV(best_model, param_grid, cv=10)
grid_search.fit(X_train, y_train)


# Step 6: Evaluation and Visualization
importance = grid_search.best_estimator_.feature_importances_
importance_dict = dict(zip(X_train.columns, importance))
print('Feature Importances:', importance_dict)

# Assuming y_test are the true values and y_pred are the predicted values
y_pred = model.predict(X_test)

# Calculate residuals
residuals = y_test - y_pred

# Scatter plot of predicted vs residuals
plt.subplot(1, 2, 1)
sns.scatterplot(x=y_pred, y=residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Residuals vs Predicted')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')

# Distribution of residuals
plt.subplot(1, 2, 2)
sns.histplot(residuals, kde=True)
plt.title('Distribution of Residuals')
plt.xlabel('Residuals')

plt.tight_layout()
plt.show()

# Calculate and print RMSE
rmse = mean_squared_error(y_test, y_pred, squared=False)
print(f'Root Mean Squared Error: {rmse}')

depths = np.arange(1, 21)
scores = []

for depth in depths:
    model = DecisionTreeRegressor(max_depth=depth)
    score = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5)
    scores.append(score.mean())

# Plotting the model complexity vs score
plt.figure()
plt.plot(depths, scores)
plt.title('Model Complexity')
plt.xlabel('Tree Depth')
plt.ylabel('Negative Mean Squared Error')
plt.grid(True)
plt.show()


# Calculating error metrics
y_pred = grid_search.best_estimator_.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print(f'Mean Absolute Error: {mae}')
print(f'Root Mean Squared Error: {rmse}')
print(comparison_df)

df = df.loc[:, (df != 0).all(axis=0)]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Assuming `model` is an instance of a decision tree regressor
model.fit(X_train, y_train)

# Now you can predict using the model
actual_values = y_test
predicted_values = model.predict(X_test)

# Creating a new data frame or using an existing one to store the actual and predicted values
df = pd.DataFrame({'Actual': actual_values, 'Predicted': predicted_values}, index=X_test.index)
df['Actual'] = actual_values
df['Predicted'] = predicted_values

# Plotting the results
plt.figure(figsize=(18, 12))
plt.plot(df.index, df['Actual'], label='Actual')
plt.plot(df.index, df['Predicted'], label='Predicted')
plt.title('Actual vs Predicted Values')
plt.xlabel('Timestamp')
plt.ylabel('Value')
plt.legend()
plt.show()

print(len(y_test))
print(len(y_pred))


y_test = y_test[:min(len(y_test), len(y_pred))]
y_pred = y_pred[:min(len(y_test), len(y_pred))]

# Plotting the actual vs predicted values
plt.figure(figsize=(18, 12))
plt.scatter(y_test, y_pred, c='crimson')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'b-')
plt.xlabel('Real Values', fontsize=14)
plt.ylabel('Predicted Values', fontsize=14)
plt.title('Real vs Predicted Values', fontsize=16)
plt.grid(True)
plt.show()


# Plotting the results
plt.figure(figsize=(16, 10))

# Sampling data points to reduce chaos; adjust as necessary
sample_rate = 10  # adjust this value based on your data
df_sampled = df[::sample_rate]

plt.plot(df_sampled.index, df_sampled['Actual'], marker='o', linestyle='-', markersize=5, label='Actual')
plt.plot(df_sampled.index, df_sampled['Predicted'], marker='x', linestyle='--', markersize=5, label='Predicted')

plt.title('Actual vs Predicted Values')
plt.xlabel('Timestamp')
plt.ylabel('Value')
plt.grid(True)
plt.legend()

plt.show()

# Assuming y_test and y_pred are lists or arrays containing your test and predicted values
comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})

# Then you can print it as you were doing
print(comparison_df)

# Plotting
plt.figure(figsize=(18,14))
plt.plot(df.index, df['Actual'], label='Actual', marker='o', markersize=4)
plt.plot(df.index, df['Predicted'], label='Predicted', marker='x', markersize=4)

# Adding annotations
for i, row in df.iterrows():
    plt.text(i, row['Actual'], f'{row["Actual"]:.2f}', fontsize=8, va='bottom', ha='right')
    plt.text(i, row['Predicted'], f'{row["Predicted"]:.2f}', fontsize=8, va='top', ha='left')

plt.title('Actual vs Predicted Values')
plt.xlabel('Timestamp')
plt.ylabel('Value')
plt.grid(True)
plt.legend()
plt.show()

print(len(y_test))
print(len(y_pred))

import matplotlib.pyplot as plt
import pandas as pd

# Assuming the data is in a DataFrame called df
data = {
    "TIMESTAMP": ["2023-01-19", "2023-11-02", "2023-03-06", "2023-08-04", "2023-05-26", "2023-01-15", "2023-01-24",
                  "2023-09-03", "2023-04-25", "2023-06-04", "2023-05-04", "2023-09-06", "2023-10-03", "2023-04-21",
                  "2023-02-14", "2023-01-16", "2023-02-20", "2023-07-05", "2023-07-04", "2023-02-25", "2023-06-23",
                  "2023-02-05", "2023-05-24", "2023-01-30", "2023-09-01", "2023-03-05", "2023-01-03", "2023-01-18",
                  "2023-06-14", "2023-05-23", "2023-06-21", "2023-02-24", "2023-03-31", "2023-03-23", "2023-07-03",
                  "2023-01-29", "2023-04-29"],
    "Actual": [251385.84925, 177021.58433, 16595.34117, 19479.96735, 35061.18713, 178542.22549, 293158.78254,
               443658.28584, 52325.14637, 40118.94963, 60737.64732, 27934.70970, 618456.46710, 77684.76790,
               361958.40109, 293078.08262, 346938.56997, 30003.41028, 24762.09387, 191311.81010, 73931.89635,
               50824.52240, 54393.06570, 302405.90121, 266211.52723, 64615.79213, 315287.41737, 350916.01949,
               45077.31608, 38700.83858, 108926.40412, 343582.57453, 78198.12139, 128649.60818, 292519.80912,
               295688.79204, 20466.83058],
    "Predicted": [337915.555479, 237091.993308, 239954.507593, 43731.997956, 32813.114677, 357578.780904, 278263.871071,
                  318693.306468, 65206.052846, 47821.443989, 50281.165136, 33600.587924, 401163.582313, 92853.744354,
                  278591.731461, 312529.231322, 401926.398583, 65816.233698, 43477.238707, 207864.656522, 78798.456457,
                  32105.372997, 54314.405627, 254260.013661, 215377.690178, 266746.175058, 304690.564855, 308150.550140,
                  46083.559993, 40804.673136, 74813.817491, 355321.750348, 89169.069945, 229005.569882, 220092.075762,
                  283367.301932, 38294.323784]
}

df = pd.DataFrame(data)
df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])
df.set_index('TIMESTAMP', inplace=True)
df.sort_index(inplace=True)

# Plotting
plt.figure(figsize=(14,10))
plt.plot(df.index, df['Actual'], label='Actual', marker='o', markersize=4)
plt.plot(df.index, df['Predicted'], label='Predicted', marker='x', markersize=4)
plt.title('Actual vs Predicted Values')
plt.xlabel('Timestamp')
plt.ylabel('Value')
plt.grid(True)
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# ... (same data preparation as above)

# Plotting
plt.figure(figsize=(18,14))
plt.plot(df.index, df['Actual'], label='Actual', marker='o', markersize=4)
plt.plot(df.index, df['Predicted'], label='Predicted', marker='x', markersize=4)

# Adding annotations
for i, row in df.iterrows():
    plt.text(i, row['Actual'], f'{row["Actual"]:.2f}', fontsize=8, va='bottom', ha='right')
    plt.text(i, row['Predicted'], f'{row["Predicted"]:.2f}', fontsize=8, va='top', ha='left')

plt.title('Actual vs Predicted Values')
plt.xlabel('Timestamp')
plt.ylabel('Value')
plt.grid(True)
plt.legend()
plt.show()



















